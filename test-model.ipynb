{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Begin preprocessing...\n",
      "INFO: Separate data by Hash. Total hash: 33515\n",
      "INFO: Progress: 33515\r"
     ]
    }
   ],
   "source": [
    "num_feature = 10\n",
    "scaler = pickle.load(open('misc/scaler.pickle', 'rb'))\n",
    "\n",
    "print('INFO: Begin preprocessing...')\n",
    "selected_col = ['hash', 'x_entry', 'y_entry', 'x_exit', 'y_exit']\n",
    "test = pd.read_csv('data_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test = test[selected_col]\n",
    "\n",
    "test_hash = test.hash.unique()\n",
    "test_ped_data = []\n",
    "\n",
    "hash_count = test_hash.shape[0]\n",
    "counter = 1\n",
    "print('INFO: Separate data by Hash. Total hash: {}'.format(hash_count))\n",
    "\n",
    "for hsh in test_hash:\n",
    "    filtered_data = test.loc[test.hash == hsh].drop('hash', axis=1).values\n",
    "\n",
    "    tmp_arr = []\n",
    "    for row in filtered_data:\n",
    "        tmp_arr.append(list(row[:2]))\n",
    "        tmp_arr.append([row[2], row[3]])\n",
    "\n",
    "    tmp_arr = np.array(tmp_arr).T\n",
    "\n",
    "    if tmp_arr.shape[1] <= num_feature:\n",
    "        count_miss = num_feature - tmp_arr.shape[1] + 1\n",
    "        arrT = tmp_arr.T\n",
    "        first_elm = np.array([arrT[0]])\n",
    "        for x in range(count_miss):\n",
    "            arrT = np.concatenate((first_elm, arrT))\n",
    "\n",
    "    elif tmp_arr.shape[1] > num_feature + 1:\n",
    "        arrT = tmp_arr.T\n",
    "        arrT = arrT[-(num_feature+1):]\n",
    "        \n",
    "    tmp_arr = arrT[:-1].T\n",
    "    test_ped_data.append(scaler.transform(tmp_arr))\n",
    "\n",
    "    print(\"INFO: Progress: {}\".format(counter), end=\"\\r\")\n",
    "    counter += 1\n",
    "\n",
    "test_ped_data = np.array(test_ped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ped_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ready_data = (test_hash, test_ped_data)\n",
    "pickle.dump(test_ready_data, open(\"misc/test_ready_data.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Test Ready data\n",
    "test_hash, test_ped_data = pickle.load(open(\"misc/test_ready_data.pickle\", \"rb\"))\n",
    "# test_ped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Model\n",
    "num_feature = 10\n",
    "rnn_size = 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(rnn_size, \n",
    "         input_shape=(2, num_feature)\n",
    "    )\n",
    ")\n",
    "model.add(Dense(2))\n",
    "model.load_weights('misc/keras_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3755459.2, -19161368. ],\n",
       "       [  3748923. , -19342528. ],\n",
       "       [  3751347. , -19234874. ],\n",
       "       [  3747429.2, -19330778. ],\n",
       "       [  3754520.8, -19211542. ],\n",
       "       [  3756649.2, -19195170. ],\n",
       "       [  3752851.8, -19248304. ],\n",
       "       [  3755731.2, -19163004. ],\n",
       "       [  3752868.2, -19208434. ],\n",
       "       [  3754711.2, -19238338. ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_Y = model.predict(test_ped_data)\n",
    "pred_Y = scaler.inverse_transform(pred_Y)\n",
    "pred_Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read trajectory id\n",
    "selected_col_traj = ['hash', 'trajectory_id', 'x_entry', 'y_entry', 'x_exit', 'y_exit']\n",
    "test_traj = pd.read_csv('data_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_traj = test_traj[selected_col].loc[test_traj.x_exit.isna()]\n",
    "\n",
    "hash_traj = {}\n",
    "for row in test_traj.values:\n",
    "    hsh = row[0]\n",
    "    traj = row[1]\n",
    "    hash_traj[hsh] = traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33514\r"
     ]
    }
   ],
   "source": [
    "f = open('misc/rnn_result.csv', 'w')\n",
    "f.write('id,target\\n')\n",
    "\n",
    "for i in range(len(pred_Y)):\n",
    "    hsh = test_hash[i]\n",
    "    traj = hash_traj[hsh]\n",
    "    x_cor = pred_Y[i][0]\n",
    "    y_cor = pred_Y[i][1]\n",
    "    \n",
    "    if (3750901.5068 <= x_cor <= 3770901.5068) and (-19268905.6133 <= y_cor <= -19208905.6133):\n",
    "        in_city = 1\n",
    "    else:\n",
    "        in_city = 0\n",
    "        \n",
    "    f.write(\"{},{}\\n\".format(traj, in_city))\n",
    "    print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test from Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_ready_data = pickle.load(open('misc/trained_ready_data.pickle', 'rb'))\n",
    "training_X, training_Y, dev_X, dev_Y, testing_X, testing_Y = trained_ready_data\n",
    "\n",
    "scaler = pickle.load(open('misc/scaler.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90584, 2, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120779, 2, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((training_X, dev_X)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120779, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((training_Y, dev_Y)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(training_X).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "num_feature = 10\n",
    "rnn_size = 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(rnn_size, \n",
    "         input_shape=(2, num_feature)\n",
    "    )\n",
    ")\n",
    "model.add(Dense(2))\n",
    "model.load_weights('misc/keras_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_X = np.array(testing_X)\n",
    "testing_Y = np.array(testing_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_Y = model.predict(testing_X[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(testing_Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(predict_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3],[6,7,8]])\n",
    "arrT = arr.T\n",
    "first_elm = arrT[0]\n",
    "for x in range(3):\n",
    "    arrT = np.concatenate((np.array([[1,6]]), arrT))\n",
    "\n",
    "arr = arrT.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.concatenate(([[1,6]], arr.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dct = dict(a=1,b=2,c=3)\n",
    "print(dct)\n",
    "\n",
    "for k, v in dct.items():\n",
    "    dct[k] = 5\n",
    "\n",
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = np.array([\n",
    "    [[1,2,3,4,5],\n",
    "     [6,7,8,9,10]],\n",
    "    [[11,12,13,14,15],\n",
    "     [16,17,18,19,20]],\n",
    "    [[21,22,23,24,25],\n",
    "     [26,27,28,29,30]]\n",
    "])\n",
    "ay = np.array([\n",
    "    [1,2],\n",
    "    [2,3],\n",
    "    [3,4]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.reshape(15,2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler = scaler.fit(np.array(range(31)).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ay_scaled = scaler.transform(ay)\n",
    "ay_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(ay.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(ay_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_2(x):\n",
    "    return x*2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = range(100)\n",
    "start_time = time.time()\n",
    "\n",
    "for x in arr:\n",
    "    print('{} {}'.format(x, times_2(x)), end='\\r')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import math\n",
    "\n",
    "\n",
    "def factorize_naive(n):\n",
    "    \"\"\" A naive factorization method. Take integer 'n', return list of\n",
    "        factors.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return []\n",
    "    factors = []\n",
    "    p = 2\n",
    "\n",
    "    while True:\n",
    "        if n == 1:\n",
    "            return factors\n",
    "\n",
    "        r = n % p\n",
    "        if r == 0:\n",
    "            factors.append(p)\n",
    "            n = n // p\n",
    "        elif p * p >= n:\n",
    "            factors.append(n)\n",
    "            return factors\n",
    "        elif p > 2:\n",
    "            # Advance in steps of 2 over odd numbers\n",
    "            p += 2\n",
    "        else:\n",
    "            # If p == 2, get to 3\n",
    "            p += 1\n",
    "    assert False, \"unreachable\"\n",
    "\n",
    "\n",
    "def chunked_worker(nums):\n",
    "    \"\"\" Factorize a list of numbers, returning a num:factors mapping.\n",
    "    \"\"\"\n",
    "    return {n: factorize_naive(n) for n in nums}\n",
    "\n",
    "\n",
    "def pool_factorizer_chunked(nums, nprocs):\n",
    "    # Manually divide the task to chunks of equal length, submitting each\n",
    "    # chunk to the pool.\n",
    "    chunksize = int(math.ceil(len(nums) / float(nprocs)))\n",
    "    futures = []\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for i in range(nprocs):\n",
    "            chunk = nums[(chunksize * i): (chunksize * (i + 1))]\n",
    "            futures.append(executor.submit(chunked_worker, chunk))\n",
    "\n",
    "    resultdict = {}\n",
    "    for f in as_completed(futures):\n",
    "        resultdict.update(f.result())\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [25, 36, 42, 88, 99]\n",
    "start_time = time.time()\n",
    "print(pool_factorizer_chunked(nums, 4))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for n in nums:\n",
    "    print(factorize_naive(n))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
